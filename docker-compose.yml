services:
  weaviate:
    image: semitechnologies/weaviate:1.26.7
    command: ["--host","0.0.0.0","--port","8080","--scheme","http"]
    ports:
      - "8080:8080"    # HTTP
      - "50051:50051"  # gRPC
    environment:
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
      QUERY_DEFAULTS_LIMIT: "25"
      ENABLE_MODULES: "text2vec-ollama,generative-ollama"
      TEXT2VEC_OLLAMA_ENDPOINT: "http://host.docker.internal:11434"
      GENERATIVE_OLLAMA_ENDPOINT: "http://host.docker.internal:11434"
      MODULES_CLIENT_TIMEOUT: "120s"
    restart: unless-stopped
    # Only needed if you're on Linux and Ollama runs on the host
    extra_hosts:
      - "host.docker.internal:host-gateway"


  # verba:
  #   image: semitechnologies/verba:latest
  #   command: ["verba","start","--host","0.0.0.0"]
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     # point to Weaviate (adjust if you use a proxy/hostname)
  #     WEAVIATE_URL_VERBA: "http://weaviate:8080"
  #     WEAVIATE_GRPC_VERBA: "weaviate:50051"
  #   depends_on:
  #     - weaviate
  #   restart: unless-stopped
